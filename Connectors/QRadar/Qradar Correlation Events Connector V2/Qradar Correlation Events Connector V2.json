{
    "isUpdateAvailable": false,
    "isCustom": false,
    "isEnabled": false,
    "isRemoteConnector": true,
    "environment": "Default Environment",
    "integration": "QRadar",
    "identifier": "Qradar Correlation Events Connector V2_c3b0b6dd-6a39-4bfb-9799-8af6944358d1",
    "connectorDefinitionName": "Qradar Correlation Events Connector V2",
    "displayName": "Qradar Correlation Events Connector V2",
    "description": "Fetches Qradar offenses and forms Chronicle SOAR (CSOAR) alerts for each Qradar rule added to dynamic list in CSOAR. Connector fetches only the offenses for rules that are added to CSOAR dynamic list. Connector requires minimum Qradar API version 10.1. Connector creates CSOAR alerts based on the rule name of Qradar offense, not the offense name.",
    "runIntervalInSeconds": 10,
    "resultDataType": 0,
    "version": "1",
    "pythonVersion": 3,
    "isAllowlistSupported": false,
    "params": [
        {
            "connectorIdentifier": null,
            "paramName": "What Value to use for the Rule Generator Field of Siemplify Alert?",
            "paramValue": "custom_rule",
            "description": "Specify what format to follow to fill the rule_generator field for the alerts created by the connector. Possible values are: custom_rule or offense_description.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Environment Field Name",
            "paramValue": "",
            "description": "Describes the name of the field where the environment name is stored. If environment field isn't found, environment is default environment",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Environment Regex Pattern",
            "paramValue": ".*",
            "description": "A regex pattern to run on the value found in the \"Environment Field Name\" field.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Use whitelist as a blacklist",
            "paramValue": "false",
            "description": "If enabled, whitelist will be used as a blacklist.",
            "type": 0,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Domain Filter",
            "paramValue": "",
            "description": "Specify qradar domains from which offenses should be ingested. If no values are provided, the connector will ingest offenses from all domains. Parameter accepts multiple values as a comma separated string.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Events Limit per Qradar Offense Rule",
            "paramValue": "100",
            "description": "Specify a limit for how many events should be ingested per single rule in Qradar offense, no new events will be ingested to the offense for the related Qradar rule once this limit is reached. Example: 100",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Events Limit for Connector to Query in One Connector Run",
            "paramValue": "",
            "description": "Specify a limit for how many events for a single offense connector should query from Qradar in one connector execution. Example: 100. Note that the value specified in the parameter can't be less than the value specified in \"Events Limit per Qradar Offense Rule\" parameter. Additionally, because of how connector fetches events, events that will be \"older\" and outside the limit will not be fetched to Siemplify, but connector will fetch newest events until the limit specified in \"Events Limit per Qradar Offense Rule\" parameter is reached.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Disable Overflow",
            "paramValue": "false",
            "description": "If enabled, connector overflow mechanism will not be checked for the created alerts - \"overflow\" alerts will not be created, connector will try to fetch all offenses returned from the Qradar.",
            "type": 0,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Script Timeout (Seconds)",
            "paramValue": "300",
            "description": "The timeout limit (in seconds) for the python process running current script",
            "type": 2,
            "mode": 0,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Proxy Server Address",
            "paramValue": "",
            "description": "The address of the proxy server to use.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Proxy Username",
            "paramValue": "",
            "description": "The proxy username to authenticate with.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Proxy Password",
            "paramValue": "",
            "description": "The proxy password to authenticate with.",
            "type": 3,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Qradar Offense Rules Re-Sync Timer",
            "paramValue": "10",
            "description": "Specify in minutes how often connector should re-sync Qradar offense rules list. If empty value or 0 is specified, connector will do re-sync every run.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Debug Logging",
            "paramValue": "false",
            "description": "If enabled, connector will write detailed messages of its execution to the log file.",
            "type": 0,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "API Root",
            "paramValue": "https://172.30.3.10",
            "description": "Qradar Server address.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "API Token",
            "paramValue": "***************",
            "description": "The API authentication token.",
            "type": 3,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "API Version",
            "paramValue": "10.1",
            "description": "The Qradar API version to be used, the Connector supports API version starting from 10.1.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Connector Events Page Size",
            "paramValue": "100",
            "description": "The size of the page that connector will use to process events in batches.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Max Offenses per Cycle",
            "paramValue": "5",
            "description": "Max offenses to process per connector run.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Events Limit per Siemplify Alert",
            "paramValue": "25",
            "description": "Max number of events to fetch per Siemplify Alert per Cycle. Can be increased to make connector run faster, if for the specified offense padding period, large numbers of events are constantly returned.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Max Hours Backwards",
            "paramValue": "24",
            "description": "Number of hours before the first connector iteration to retrieve incidents from. This parameter applies to the initial connector iteration after you enable the connector for the first time, or used as a fallback value in cases where connector's last run timestamp expires.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Offenses Padding Period",
            "paramValue": "60",
            "description": "Time frame in minutes to fetch offenses in minutes.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Events Padding Period",
            "paramValue": "1",
            "description": "Time frame in days to fetch events data.",
            "type": 1,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": true,
            "isAdvanced": false,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "Custom Fields",
            "paramValue": "kmproperty",
            "description": "Custom fields that configured by the user at the QRadar, comma separated, e.g: FieldA,Field B",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        },
        {
            "connectorIdentifier": null,
            "paramName": "What Value to use for the Name Field of Siemplify Alert?",
            "paramValue": "custom_rule",
            "description": "Specify what format to follow to generate names for the alerts created by the connector. Possible values are: custom_rule or offense_description.",
            "type": 2,
            "mode": 2,
            "isDisplayed": true,
            "isMandatory": false,
            "isAdvanced": true,
            "id": 0,
            "creationTimeUnixTimeInMs": 0,
            "modificationTimeUnixTimeInMs": 0
        }
    ],
    "allowList": [
        "kmrule3"
    ],
    "integrationVersion": 60.0,
    "isScriptConnector": true,
    "script": "import sys\nimport json\nimport copy\nimport uuid\nimport hashlib\nimport arrow\nfrom SiemplifyConnectorsDataModel import AlertInfo\nfrom SiemplifyUtils import (\n    output_handler,\n    convert_datetime_to_unix_time,\n    convert_unixtime_to_datetime,\n    unix_now,\n)\nfrom TIPCommon import (\n    extract_connector_param,\n    siemplify_fetch_timestamp,\n    validate_timestamp,\n    string_to_multi_value,\n    read_content,\n    WHITELIST_FILTER,\n    BLACKLIST_FILTER,\n    TIMEOUT_THRESHOLD,\n)\nfrom BaseIntegrationClasses import BaseConnector, QRadarLoggerWrapper\nfrom QRadarCommon import QRadarCommon\nfrom exceptions import (\n    QRadarConnectorValidationException,\n    QRadarCustomFieldValidation,\n    QRadarInvalidRuleException,\n)\nfrom constants import (\n    QRADAR_CORRELATION_EVENTS_CONNECTOR_V2_SCRIPT_NAME,\n    DEFAULT_DOMAIN,\n    RULE_ID_NAME_MAPPING_DB_KEY,\n    RULE_ID_NAME_MAPPING_FILE,\n    CORRELATIONS_CONNECTOR_V2_NAME,\n    DEFAULT_ORDER_BY_KEY,\n    DEFAULT_SORT_ORDER,\n)\nfrom UtilsManager import load_offense_events, save_offense_events, create_rule_mapping\n\n\n# CONSTANTS\nALERT_REQUIRED_FIELDS = (\"custom_rule\", \"offense_description\")\nDEFAULT_MAX_HOURS_BACKWARDS = 24\nSTOPPED_TIMER = -1\nFAILED_TO_FETCH_EVENTS = \"Cannot Fetch Events for the Offense\"\nONE_MINUTE_UNIX_TIMESTAMP = 60_000\n\n\nclass QradarCorrelationEventsConnectorV2(BaseConnector):\n    def __init__(self):\n        \"\"\"\n        Create an instance of the connector\n        \"\"\"\n        self.common = None\n\n        BaseConnector.__init__(self, QRADAR_CORRELATION_EVENTS_CONNECTOR_V2_SCRIPT_NAME)\n        self.rule_names_mapping = {}\n        self.offense_events = {}\n        # This param will hold the new timestamp for the connector to save\n        # The param will be updated to the last_updated_time of the last processed offense\n\n    def set_connector_logger(self):\n        self.logger = QRadarLoggerWrapper(\n            self.logger, debug_logging=self.params.debug_logging\n        )\n\n    def load_manager(self):\n        if float(self.params.api_version) >= 10.1:\n            from QRadarManagerV10 import QRadarV10Manager\n\n            return QRadarV10Manager(\n                self.params.api_root,\n                self.params.api_token,\n                self.params.api_version,\n                logger=self.logger,\n            )\n        else:\n            from QRadarManager import QRadarManager\n\n            return QRadarManager(\n                self.params.api_root, self.params.api_token, self.params.api_version\n            )\n\n    def load_common_classes(self):\n        \"\"\"\n        Load the common classes of the connector\n        \"\"\"\n        self.common = QRadarCommon()\n\n    def load_connector_configuration(self):\n        \"\"\"\n        Load the connector configurations\n        \"\"\"\n        self.params.python_process_timeout = extract_connector_param(\n            siemplify=self.siemplify,\n            param_name=\"PythonProcessTimeout\",\n            input_type=int,\n            is_mandatory=True,\n            print_value=True,\n        )\n\n        self.params.api_root = extract_connector_param(\n            self.siemplify, param_name=\"API Root\", is_mandatory=True, print_value=True\n        )\n\n        self.params.api_token = extract_connector_param(\n            self.siemplify, param_name=\"API Token\", is_mandatory=True, print_value=False\n        )\n\n        self.params.api_version = extract_connector_param(\n            self.siemplify, param_name=\"API Version\", print_value=True\n        )\n\n        self.params.events_page_size = extract_connector_param(\n            self.siemplify,\n            param_name=\"Connector Events Page Size\",\n            is_mandatory=True,\n            input_type=int,\n            print_value=True,\n        )\n\n        self.params.offenses_limit_per_cycle = extract_connector_param(\n            self.siemplify,\n            param_name=\"Max Offenses per Cycle\",\n            is_mandatory=True,\n            input_type=int,\n            print_value=True,\n        )\n\n        self.params.events_limit_per_alert = extract_connector_param(\n            self.siemplify,\n            param_name=\"Events Limit per Siemplify Alert\",\n            is_mandatory=True,\n            input_type=int,\n            print_value=True,\n        )\n\n        self.params.offenses_padding_period = extract_connector_param(\n            self.siemplify,\n            param_name=\"Offenses Padding Period\",\n            is_mandatory=True,\n            input_type=int,\n            print_value=True,\n        )\n\n        self.params.events_padding_period = extract_connector_param(\n            self.siemplify,\n            param_name=\"Events Padding Period\",\n            is_mandatory=True,\n            input_type=int,\n            print_value=True,\n        )\n\n        self.params.max_hours_backwards = extract_connector_param(\n            self.siemplify,\n            param_name=\"Max Hours Backwards\",\n            is_mandatory=False,\n            input_type=int,\n            default_value=DEFAULT_MAX_HOURS_BACKWARDS,\n            print_value=True,\n        )\n\n        self.params.custom_fields = extract_connector_param(\n            self.siemplify, param_name=\"Custom Fields\", print_value=True\n        )\n\n        self.params.environment_field_name = extract_connector_param(\n            self.siemplify, param_name=\"Environment Field Name\", print_value=True\n        )\n\n        self.params.environment_regex = extract_connector_param(\n            self.siemplify, param_name=\"Environment Regex Pattern\", print_value=True\n        )\n\n        self.params.whitelist_as_blacklist = extract_connector_param(\n            self.siemplify,\n            param_name=\"Use whitelist as a blacklist\",\n            print_value=True,\n            is_mandatory=True,\n            default_value=False,\n            input_type=bool,\n        )\n\n        self.params.whitelist_filter_type = (\n            BLACKLIST_FILTER if self.params.whitelist_as_blacklist else WHITELIST_FILTER\n        )\n\n        self.params.disable_overflow = extract_connector_param(\n            self.siemplify,\n            param_name=\"Disable Overflow\",\n            print_value=True,\n            default_value=False,\n            input_type=bool,\n        )\n\n        self.params.offense_rule_events_limit = extract_connector_param(\n            self.siemplify,\n            param_name=\"Events Limit per Qradar Offense Rule\",\n            print_value=True,\n            input_type=int,\n        )\n\n        self.params.events_query_limit = extract_connector_param(\n            self.siemplify,\n            param_name=\"Events Limit for Connector to Query in One Connector Run\",\n            print_value=True,\n            input_type=int,\n        )\n\n        self.params.alert_name_field_name = extract_connector_param(\n            self.siemplify,\n            param_name=\"What Value to use for the Name Field of Siemplify Alert?\",\n            print_value=True,\n        )\n\n        self.params.rule_generator_field_name = extract_connector_param(\n            self.siemplify,\n            param_name=\"What Value to use for the Rule Generator Field of Siemplify Alert?\",\n            print_value=True,\n        )\n        self.params.domain_filter = extract_connector_param(\n            self.siemplify,\n            param_name=\"Domain Filter\",\n            is_mandatory=False,\n            print_value=True,\n        )\n        self.params.domain_filter_list = list(\n            set(string_to_multi_value(self.params.domain_filter))\n        )\n\n        self.params.qradar_offense_rules_resync_timer = extract_connector_param(\n            self.siemplify,\n            param_name=\"Qradar Offense Rules Re-Sync Timer\",\n            print_value=True,\n            input_type=int,\n            default_value=0,\n        )\n        self.validate_positive_integer_params(\n            self.params.qradar_offense_rules_resync_timer,\n            \"Qradar Offense Rules Re-Sync Timer\",\n            can_be_zero=True,\n        )\n\n        self.params.debug_logging = extract_connector_param(\n            self.siemplify,\n            param_name=\"Debug Logging\",\n            print_value=True,\n            input_type=bool,\n            default_value=False,\n        )\n\n        self.params.create_empty_cases = False\n\n        self.validate_alert_name_field_name()\n        self.validate_rule_generator_field_name()\n        self.validate_events_limit_per_rule()\n\n    def validate_positive_integer_params(\n        self, input_param, name: str, can_be_zero: bool\n    ):\n        if input_param is not None:\n            if not can_be_zero and input_param == 0:\n                raise QRadarConnectorValidationException(\n                    f\"'{name}' must be a number bigger than 0\"\n                )\n        if input_param < 0:\n            raise QRadarConnectorValidationException(\n                f\"'{name}' must be a number bigger or equal to 0\"\n            )\n\n    def validate_events_limit_per_rule(self):\n        \"\"\"\n        Validate rule events limit parameters\n        return: {void} exception in case of inconsistent limits\n        \"\"\"\n        if (\n            self.params.offense_rule_events_limit\n            and self.params.events_query_limit\n            and self.params.events_query_limit < self.params.offense_rule_events_limit\n        ):\n            self.logger.error(\n                'Value provided for the \"Events Limit per Qradar Offense Rule\" can\\'t be bigger than '\n                'value for \"Events Limit for Connector to Query in One Connector Run\"'\n            )\n            raise QRadarConnectorValidationException(\n                'Value provided for the \"Events Limit per Qradar Offense Rule\" '\n                \"can't be bigger than value for \\\"Events Limit for Connector to \"\n                'Query in One Connector Run\"'\n            )\n\n    def load_rule_names_mappings(self):\n        \"\"\"\n        Load rule names to ID mapping from file (or generate new mapping if needed)\n        :return: {dict} The rule names mapping, in the following format:\n        {\n          \"rules_id_name_mapping\":{\n            \"latest_whitelist_hashsum\":\"43c94f5b6a0b1202c118a874c626e461\",\n            \"is_whitelist_as_blacklist\": true,\n            \"last_update_timestamp\": unixtime\n            \"mapping\":{\n                \"100224\":\"Local: SSH or Telnet Detected on Non-Standard Port\",\n                \"100051\":\"Multiple Login Failures from the Same Source\",\n                \"100045\":\"AssetExclusion: Exclude NetBIOS Name By MAC Address\",\n                \"100046\":\"Login Failure to Disabled Account\",\n                \"100205\":\"Destination Network Weight is Low\",\n                \"100211\":\"Source Network Weight is Low\",\n                \"100209\":\"Context Is Local to QRADAR _ DONT USE\"\n            }\n          }\n        }\n        \"\"\"\n        if self.rule_names_mapping:\n            self.logger.debug(\"Rule names mapping's already fetched.\")\n            return\n\n        current_mapping = read_content(\n            self.siemplify,\n            RULE_ID_NAME_MAPPING_FILE,\n            RULE_ID_NAME_MAPPING_DB_KEY,\n            {\"is_empty\": \"true\"},\n        )\n\n        if current_mapping == {\"is_empty\": \"true\"}:\n            self.logger.info(\n                \"Rule names mapping file doesn't exist. Creating new rule names mapping.\"\n            )\n            current_mapping = create_rule_mapping(\n                siemplify=self.siemplify,\n                is_whitelist_as_blacklist=self.is_whitelist_as_blacklist(),\n                calculate_hash=self.calculate_whitelist_hash(),\n                rules=self.manager.list_rules(),\n                connector_name=CORRELATIONS_CONNECTOR_V2_NAME,\n                logger=self.logger,\n            )\n        else:\n            try:\n                mapping_expired = (\n                    unix_now()\n                    - current_mapping.get(\"rules_id_name_mapping\", {}).get(\n                        \"last_update_timestamp\"\n                    )\n                    > self.params.qradar_offense_rules_resync_timer\n                    * ONE_MINUTE_UNIX_TIMESTAMP\n                )\n                if self.is_whitelist_changed(\n                    current_mapping.get(\"rules_id_name_mapping\", {}).get(\n                        \"latest_whitelist_hashsum\"\n                    )\n                ):\n                    self.logger.info(\n                        \"Whitelist has changed. Creating new rule names mapping.\"\n                    )\n                    current_mapping = create_rule_mapping(\n                        siemplify=self.siemplify,\n                        is_whitelist_as_blacklist=self.is_whitelist_as_blacklist(),\n                        calculate_hash=self.calculate_whitelist_hash(),\n                        rules=self.manager.list_rules(),\n                        connector_name=CORRELATIONS_CONNECTOR_V2_NAME,\n                        logger=self.logger,\n                    )\n                elif self.is_whitelist_as_blacklist_changed(\n                    current_mapping.get(\"rules_id_name_mapping\", {}).get(\n                        \"is_whitelist_as_blacklist\", False\n                    )\n                ):\n                    self.logger.info(\n                        '\"Use whitelist as a blacklist\" parameter was changed. Creating new rule names mapping.'\n                    )\n                    current_mapping = create_rule_mapping(\n                        siemplify=self.siemplify,\n                        is_whitelist_as_blacklist=self.is_whitelist_as_blacklist(),\n                        calculate_hash=self.calculate_whitelist_hash(),\n                        rules=self.manager.list_rules(),\n                        connector_name=CORRELATIONS_CONNECTOR_V2_NAME,\n                        logger=self.logger,\n                    )\n\n                elif mapping_expired:\n                    self.logger.info(\n                        f\"More then {self.params.qradar_offense_rules_resync_timer} minutes past since \"\n                        f\"the last update of the rule mapping. Creating new rule mapping\"\n                    )\n                    current_mapping = create_rule_mapping(\n                        siemplify=self.siemplify,\n                        is_whitelist_as_blacklist=self.is_whitelist_as_blacklist(),\n                        calculate_hash=self.calculate_whitelist_hash(),\n                        rules=self.manager.list_rules(),\n                        connector_name=CORRELATIONS_CONNECTOR_V2_NAME,\n                        logger=self.logger,\n                    )\n            except Exception as e:\n\n                if isinstance(e, QRadarInvalidRuleException):\n                    raise Exception(\n                        \"Connector failed to run because the  offense rule(s) provided in the whitelist\"\n                        \" section (dynamic list) is (are) not valid.\"\n                    )\n                self.logger.error(f\"Unable to read rule name mappings file: {e}\")\n                self.logger.exception(e)\n                self.logger.info(\"Creating new rule names mapping\")\n                current_mapping = create_rule_mapping(\n                    siemplify=self.siemplify,\n                    is_whitelist_as_blacklist=self.is_whitelist_as_blacklist(),\n                    calculate_hash=self.calculate_whitelist_hash(),\n                    rules=self.manager.list_rules(),\n                    connector_name=CORRELATIONS_CONNECTOR_V2_NAME,\n                    logger=self.logger,\n                )\n\n        if not current_mapping[\"rules_id_name_mapping\"][\"mapping\"]:\n            self.logger.error(\n                \"No valid rules found in the whitelist. Please add rules. Aborting\"\n            )\n            raise Exception(\n                \"No valid rules found in the whitelist. Please add rules. Aborting\"\n            )\n\n        self.rule_names_mapping = current_mapping\n\n    def validate_alert_name_field_name(self):\n        \"\"\"\n        Validate the value passed to the Name Field of Siemplify Alert configuration\n        :return: {bool} True if valid, exception otherwise\n        \"\"\"\n        if self.params.alert_name_field_name not in ALERT_REQUIRED_FIELDS:\n            raise QRadarConnectorValidationException(\n                f'Valid values to use for the Name Field of Siemplify Alert are {\" or \".join(ALERT_REQUIRED_FIELDS)}'\n            )\n\n        return True\n\n    def validate_rule_generator_field_name(self):\n        \"\"\"\n        Validate the value passed to the Rule Generator Field of Siemplify Alert configuration\n        :return: {bool} True if valid, exception otherwise\n        \"\"\"\n        if self.params.rule_generator_field_name not in ALERT_REQUIRED_FIELDS:\n            raise QRadarConnectorValidationException(\n                f'Valid values to use for the Rule Generator Field of Siemplify Alert are {\" or \".join(ALERT_REQUIRED_FIELDS)}'\n            )\n\n        return True\n\n    def alert_id_repr(self, offense):\n        \"\"\"\n        Get the alert's ID representation (in this case - the ID of the offense)\n        :param offense: {Offense} The offense\n        :return: {int} The offense ID\n        \"\"\"\n        return offense.id\n\n    def calculate_whitelist_hash(self):\n        \"\"\"\n        Calculate the current whitelist md5 hash\n        :return: {unicode} The md5 of the whitelist\n        \"\"\"\n        return hashlib.md5(\n            json.dumps(sorted(self.siemplify.whitelist)).encode()\n        ).hexdigest()\n\n    def is_whitelist_as_blacklist(self):\n        \"\"\"\n        Check if whitelist is used as blacklist\n        :return: {bool} True if whitelist is used as blacklist, otherwise False\n        \"\"\"\n        return bool(self.params.whitelist_filter_type == BLACKLIST_FILTER)\n\n    def is_whitelist_changed(self, current_hash):\n        \"\"\"\n        Check if the whitelist has changed from previous runs by comparing its hash\n        :param current_hash: {unicode} The currently saved hash of the whitelist (saved in previous run)\n        :return: {bool} True if changed, otherwise False.\n        \"\"\"\n        return current_hash != self.calculate_whitelist_hash()\n\n    def is_whitelist_as_blacklist_changed(self, current_whitelist_as_blacklist_flag):\n        \"\"\"\n        Check if \"Use whitelist as a blacklist\" parameter was changed from previous runs\n        :param current_whitelist_as_blacklist_flag: {bool} The currently saved whitelist as blacklist boolean flag (saved in previous run)\n        :return: {bool} True if changed, otherwise False.\n        \"\"\"\n        return current_whitelist_as_blacklist_flag != self.params.whitelist_as_blacklist\n\n    def fetch_last_success_time(self):\n        \"\"\"\n        Fetch the last success time of the connector.\n        The last success time will be either the last updated time of the newest processed offense,\n        ot (NOW - max_hours_backwards param)\n        :return:\n        \"\"\"\n        try:\n            saved_timestamp = siemplify_fetch_timestamp(\n                siemplify=self.siemplify, datetime_format=True\n            )\n        except Exception as e:\n            self.logger.error(\n                \"An error as occurred while fetching saved timestamp. Resetting timestamp.\"\n            )\n            self.logger.exception(e)\n            saved_timestamp = convert_unixtime_to_datetime(1)\n\n        return validate_timestamp(\n            saved_timestamp, offset_in_hours=self.params.max_hours_backwards\n        )\n\n    def pre_processing(self):\n        \"\"\"\n        Perform some pre-processing in the connector\n        \"\"\"\n        # Load rule names mapping\n        self.load_rule_names_mappings()\n        # Load already seen events hashes\n        self.offense_events = load_offense_events(\n            self.siemplify, self.params.events_padding_period\n        )\n\n        # If the offense padding period has changed or last timestamp is earlier than the padding period (means that\n        # either the offenses were not updated in a long time, or the connector has stopped for a certain period) -\n        # reset all the timers to avoid false positive empty cases\n        if self.is_offense_padding_changed():\n            self.logger.info(\"Offense padding has changed. Resetting all timers.\")\n            self.reset_timers()\n\n        # Add +1 to padding period to make sure that if a real timer has really exceeded, we will create an empty case\n        # for it, and it won't be missed because of this condition.\n        elif self.last_success_time_datetime < arrow.utcnow().shift(\n            minutes=-self.params.offenses_padding_period + 1\n        ):\n            self.logger.info(\n                \"Last success time of connector is older than the offense padding period. Resetting all timers\"\n            )\n            self.reset_timers()\n\n        self.logger.info(\n            \"Stopping timers for CLOSED offenses that were updated in the offense padding period.\"\n        )\n\n        closed_updated_offenses = self.get_updated_closed_offenses()\n        # Get only the closed offenses that have active timers\n        closed_updated_offenses = [\n            offense\n            for offense in closed_updated_offenses\n            if not self.is_timer_stopped(offense.id)\n        ]\n\n        self.logger.info(\n            f\"Found {len(closed_updated_offenses)} updated offenses that are CLOSED and have active timers.\"\n        )\n\n        for closed_offense in closed_updated_offenses:\n            self.logger.info(\n                f\"Offense {closed_offense.id} was updated and is CLOSED. Stopping its timer.\"\n            )\n            self.stop_offense_timer(closed_offense.id)\n\n    def post_processing(self):\n        \"\"\"\n        Perform some post-processing in the connector\n        \"\"\"\n        # Save the updated events hashes to the offense events file\n        self.save_filtered_offense_events()\n\n    def save_event_to_offense_events(self, offense, rule_id, event):\n        \"\"\"\n        Save an event to the offense_events.json file (mark event as already seen to avoid duplicates)\n        :param offense: {Offense} The offense of the event\n        :param rule_id: {int} The ID of the rule that the event triggered\n        :param event: {Event} The event to save\n        \"\"\"\n        event_hash = event.as_hash()\n        offense_id = str(offense.id)\n        rule_id = str(rule_id)\n\n        # If the offense is new\n        if offense_id not in list(self.offense_events[\"offenses\"].keys()):\n            # Offense was never in the offense events file. Need to create new record\n            self.logger.debug(\n                f\"Offense {offense_id} is new for offenses file, creating an entry.\"\n            )\n            self.offense_events[\"offenses\"][offense_id] = {\n                \"last_update_time\": offense.last_updated_time,\n                \"no_new_events_timer_start_time\": STOPPED_TIMER,\n                \"rules\": {rule_id: {\"events\": {event_hash: unix_now()}}},\n            }\n        # If the rule is new\n        elif rule_id not in self.offense_events[\"offenses\"][offense_id][\"rules\"]:\n            self.logger.debug(f\"Rule {rule_id} is new for offense {offense_id}\")\n            self.offense_events[\"offenses\"][offense_id][\"rules\"][rule_id] = {\n                \"events\": {event_hash: unix_now()}\n            }\n        # If the event is new\n        elif (\n            event_hash\n            not in self.offense_events[\"offenses\"][offense_id][\"rules\"][rule_id][\n                \"events\"\n            ]\n        ):\n            self.logger.debug(\n                f\"Event with hash {event_hash} is new for offense {offense_id}\"\n            )\n            # Event is seen for the first time for the offense\n            self.offense_events[\"offenses\"][offense_id][\"rules\"][rule_id][\"events\"][\n                event_hash\n            ] = unix_now()\n\n        # Update the last_update_time of the offense\n        self.offense_events[\"offenses\"][offense_id][\n            \"last_update_time\"\n        ] = offense.last_updated_time\n\n        if not self.offense_events[\"offenses\"][offense_id].get(\n            \"total_events_collected_per_rule\"\n        ):\n            self.offense_events[\"offenses\"][offense_id][\n                \"total_events_collected_per_rule\"\n            ] = {}\n\n        total = (\n            self.offense_events[\"offenses\"][offense_id][\n                \"total_events_collected_per_rule\"\n            ].get(rule_id)\n            or 0\n        )\n        self.offense_events[\"offenses\"][offense_id][\"total_events_collected_per_rule\"][\n            rule_id\n        ] = (total + 1)\n\n    def save_filtered_offense_events(self):\n        \"\"\"\n        Save the offense events file\n        \"\"\"\n        # Before writing the new events - filter the old events hashes\n        self.logger.debug(\"Filtering old events hashes\")\n        self.offense_events = self.filter_old_offense_events()\n        # Update the last_offense_padding_period to the current's run offense padding period\n        self.offense_events[\"last_offense_padding_period\"] = (\n            self.params.offenses_padding_period\n        )\n        # Saving events to db\\file dynamically\n        save_offense_events(\n            self.siemplify, self.params.offenses_padding_period, self.offense_events\n        )\n\n    def filter_old_offense_events(self):\n        \"\"\"\n        Filter old events hashes from the offense events to prevent the file from getting too big\n        :return: {dict} Filtered offense events dict\n        \"\"\"\n        filtered_offense_events = copy.deepcopy(self.offense_events)\n\n        # Calculate the time limit beyond which an event hash will be considered old\n        # The limit is equal to NOW - 2 * max(events_padding_period, max_hours_backwards) to guarantee that a needed hash\n        # won't be deleted too soon\n        time_limit = (\n            arrow.utcnow()\n            .shift(\n                days=-max(\n                    self.params.events_padding_period,\n                    self.params.max_hours_backwards / 24,\n                )\n            )\n            .int_timestamp\n        ) * 1000\n\n        # In each offense id\n        for offense_id in map(str, self.offense_events[\"offenses\"].keys()):\n            # In each rule\n            for rule_id in self.offense_events[\"offenses\"][offense_id][\"rules\"]:\n                # In each event item\n                for event_hash, timestamp in self.offense_events[\"offenses\"][\n                    offense_id\n                ][\"rules\"][rule_id][\"events\"].items():\n                    if timestamp < time_limit:\n                        del filtered_offense_events[\"offenses\"][offense_id][\"rules\"][\n                            rule_id\n                        ][\"events\"][event_hash]\n\n        return filtered_offense_events\n\n    def start_offense_timer(self, offense_id, restart_timer=False):\n        \"\"\"\n        Start the \"failed to fetch events\" timer for the given offense.\n        A timer should be started in one of the following cases:\n        - An exception occurred while fetching events for an offense (any rule)\n        - No events fetched at all for the offense (from all ruled together)\n        If a timer was already started for the offense - nothing will be done unless restart_timer is set to True\n        :param offense_id: {int} The ID of the offense to start the timer for\n        :param restart_timer: {bool} Override and restart existing started timer\n        \"\"\"\n        offense_id = str(offense_id)\n\n        if offense_id not in list(self.offense_events[\"offenses\"].keys()):\n            # Offense was never in the offense events file. Need to create new record\n            self.offense_events[\"offenses\"][offense_id] = {\n                \"last_update_time\": 0,\n                \"no_new_events_timer_start_time\": unix_now(),\n                \"rules\": {},\n            }\n\n        if (\n            restart_timer\n            or self.offense_events[\"offenses\"][offense_id][\n                \"no_new_events_timer_start_time\"\n            ]\n            == STOPPED_TIMER\n        ):\n            # Only start new timer if there is timer is not already started, or if restart timer is True\n            self.offense_events[\"offenses\"][offense_id][\n                \"no_new_events_timer_start_time\"\n            ] = unix_now()\n\n    def stop_offense_timer(self, offense_id):\n        \"\"\"\n        End the \"failed to fetch events\" timer for the given offense.\n        A timer should be stopped if:\n        - Offense Padding Period has changed since last connector run\n        - Events were successfully fetched for at least one rule\n        - Timer exceeded and empty AlertInfo was created\n        :param offense_id: {int} The ID of the offense to end the timer for\n        \"\"\"\n        offense_id = str(offense_id)\n\n        if offense_id not in list(self.offense_events[\"offenses\"].keys()):\n            # Offense was never in the offense events file. Need to create new record\n            self.offense_events[\"offenses\"][offense_id] = {\n                \"last_update_time\": 0,\n                \"no_new_events_timer_start_time\": STOPPED_TIMER,\n                \"rules\": {},\n            }\n\n        else:\n            self.offense_events[\"offenses\"][offense_id][\n                \"no_new_events_timer_start_time\"\n            ] = STOPPED_TIMER\n\n    def reset_timers(self):\n        \"\"\"\n        Reset all \"failed to fetch events\" timers for all offenses\n        \"\"\"\n        for offense_id in map(str, self.offense_events[\"offenses\"].keys()):\n            self.offense_events[\"offenses\"][offense_id][\n                \"no_new_events_timer_start_time\"\n            ] = STOPPED_TIMER\n\n    def is_offense_padding_changed(self):\n        \"\"\"\n        Check if Offense Padding Period param has changed since previous run\n        :return: {bool} True if changed, False otherwise\n        \"\"\"\n        if not self.offense_events:\n            self.offense_events = load_offense_events(\n                self.siemplify, self.params.events_padding_period\n            )\n\n        return self.params.offenses_padding_period != int(\n            self.offense_events[\"last_offense_padding_period\"]\n        )\n\n    def is_timer_stopped(self, offense_id):\n        \"\"\"\n        Check if the timer of a given offense is stopped or not\n        :param offense_id: {int} The ID of the offense\n        :return: {bool} True if stopped, False otherwise\n        \"\"\"\n        offense_id = str(offense_id)\n\n        if offense_id not in self.offense_events[\"offenses\"]:\n            # No timer was initialized ever\n            return True\n\n        if (\n            self.offense_events[\"offenses\"][offense_id][\n                \"no_new_events_timer_start_time\"\n            ]\n            == STOPPED_TIMER\n        ):\n            # Timer is stopped\n            return True\n\n        return False\n\n    def is_timer_exceeded(self, offense_id):\n        \"\"\"\n        Check if the timer of a given offense has timed out or not\n        :param offense_id: {int} The ID of the offense\n        :return: {bool} True if timed out, False otherwise\n        \"\"\"\n        offense_id = str(offense_id)\n\n        if offense_id not in self.offense_events[\"offenses\"]:\n            # No timer was initialized ever\n            return False\n\n        if (\n            self.offense_events[\"offenses\"][offense_id][\n                \"no_new_events_timer_start_time\"\n            ]\n            == STOPPED_TIMER\n        ):\n            # Timer is stopped\n            return False\n\n        # If the no_new_events_timer_start_time timestamp of the offense is smaller (earlier) than (NOW - offenses_padding_period)\n        # then the timer has exceeded\n        return (\n            self.offense_events[\"offenses\"][offense_id][\n                \"no_new_events_timer_start_time\"\n            ]\n            < arrow.utcnow()\n            .shift(minutes=-self.params.offenses_padding_period)\n            .int_timestamp\n            * 1000\n        )\n\n    def get_timed_out_offenses_ids(self):\n        \"\"\"\n        Get the IDS of the offenses that timed out\n        :return: {list} List of IDS of timed out offenses\n        \"\"\"\n        offense_ids = []\n        for offense_id in list(self.offense_events[\"offenses\"].keys()):\n            if self.is_timer_exceeded(offense_id):\n                offense_ids.append(offense_id)\n\n        return offense_ids\n\n    def process_timed_out_offenses(self):\n        \"\"\"\n        Process timed out offenses - create an empty AlertInfo for them and stop their timers\n        :return: {[AlertInfo]} The created empty AlertInfo objects\n        \"\"\"\n        alerts = []\n        for offense_id in self.get_timed_out_offenses_ids():\n            try:\n                self.logger.info(\n                    f'Offense {offense_id} \"Failed to fetch events\" timer has timed out. Creating empty AlertInfo.'\n                )\n                alert_info = self.create_failed_to_fetch_events_alert_info(offense_id)\n                alerts.append(alert_info)\n                self.stop_offense_timer(offense_id)\n\n            except Exception as e:\n                self.logger.error(f\"Failed to process timed out offense {offense_id}\")\n                self.logger.exception(e)\n\n        return alerts\n\n    def get_updated_closed_offenses(self):\n        \"\"\"\n        Get updated offenses from QRadar in the matching searching period that are closed.\n        The searching period is determined by the offenses_padding_period and the last success time of the connector.\n        :return: {list} The found closed updated offenses, up to offenses_limit_per_cycle param limit\n        \"\"\"\n        # Pick the earlier between (NOW - offenses_padding_period) and last success time (up to Max Days Backwards)\n        last_hour = (\n            arrow.utcnow().shift(minutes=-self.params.offenses_padding_period).datetime\n        )\n        fetch_time = min(self.last_success_time_datetime, last_hour)\n\n        self.logger.info(\n            f\"Fetching closed updated offenses since {fetch_time.isoformat()}\"\n        )\n        # Fetch the updated offenses in the search period, sorted by last update time\n        return self.manager.get_updated_offenses_from_time(\n            timestamp_unix_time=convert_datetime_to_unix_time(fetch_time),\n            status=\"CLOSED\",\n            connector_starting_time=self.connector_starting_time,\n            python_process_timeout=self.params.python_process_timeout,\n        )\n\n    def get_alerts_to_process(self):\n        \"\"\"\n        Get updated offenses from QRadar in the matching searching period.\n        The searching period is determined by the offenses_padding_period and the last success time of the connector.\n        :return: {list} The found updated offenses, up to offenses_limit_per_cycle param limit\n        \"\"\"\n        # Pick the earlier between (NOW - offenses_padding_period) and last success time (up to Max Days Backwards)\n        last_hour = (\n            arrow.utcnow().shift(minutes=-self.params.offenses_padding_period).datetime\n        )\n        fetch_time = min(self.last_success_time_datetime, last_hour)\n        domain_ids = []\n        self.logger.info(f\"Fetching updated offenses since {fetch_time.isoformat()}\")\n        # Fetch the updated offenses in the search period, sorted by last update time\n\n        if self.params.domain_filter_list:\n            all_domains = self.manager.get_domains()\n            domains_name_to_id = {domain.name: domain.id for domain in all_domains}\n            invalid_domains = []\n            for domain in self.params.domain_filter_list:\n                if domain == DEFAULT_DOMAIN:\n                    domain_ids.append(0)\n                elif domain in domains_name_to_id:\n                    domain_ids.append(domains_name_to_id[domain])\n                else:\n                    invalid_domains.append(domain)\n            if invalid_domains:\n                raise QRadarConnectorValidationException(\n                    f\"Following values for \\\"Domain Filter\\\" parameter were not found in QRadar: {', '.join(invalid_domains)}.\"\n                )\n\n        return self.manager.get_updated_offenses_from_time(\n            timestamp_unix_time=convert_datetime_to_unix_time(fetch_time),\n            domain_ids=domain_ids,\n            connector_starting_time=self.connector_starting_time,\n            python_process_timeout=self.params.python_process_timeout,\n        )\n\n    def get_rule_name_by_id(self, rule_id):\n        \"\"\"\n        Get rule name by the rule ID\n        :param rule_id: {int} The rule ID\n        :return: {unicode} The rule name\n        \"\"\"\n        self.load_rule_names_mappings()\n\n        return (\n            self.rule_names_mapping.get(\"rules_id_name_mapping\", {})\n            .get(\"mapping\", {})\n            .get(str(rule_id))\n        )\n\n    def get_already_seen_events_hashes(self, offense, rule_id):\n        \"\"\"\n        Get the already seen events hashes for a given offense and rule id\n        :param offense: {Offense} The offense to get the hashes for\n        :param rule_id: {int} The rule id to get the events for\n        :return: {[unicode]} List of already seen hashes\n        \"\"\"\n        offense_id = str(offense.id)\n        rule_id = str(rule_id)\n        return (\n            self.offense_events[\"offenses\"]\n            .get(offense_id, {})\n            .get(\"rules\", {})\n            .get(rule_id, {})\n            .get(\"events\", {})\n            .keys()\n        )\n\n    def is_whitelisted_rule_id(self, rule_id):\n        \"\"\"\n        Check if a given rule ID is whitelisted or not (by lookup in the rules mapping)\n        :param rule_id: {int} The ID of the rule\n        :return: {bool} True if whitelisted, False otherwise.\n        \"\"\"\n        return str(rule_id) in list(\n            self.rule_names_mapping.get(\"rules_id_name_mapping\", {})\n            .get(\"mapping\", {})\n            .keys()\n        )\n\n    def is_rule_id_passed_whitelist_filter(self, rule_id):\n        \"\"\"\n        Check if a given rule ID passed whitelist filters.\n        :param rule_id: {int} The ID of the rule\n        :return: {bool} True if passed whitelist, otherwise False\n        \"\"\"\n        return str(rule_id) in list(\n            self.rule_names_mapping.get(\"rules_id_name_mapping\", {})\n            .get(\"mapping\", {})\n            .keys()\n        )\n\n    def is_whitelisted_alert(self, offense):\n        \"\"\"\n        Check if the offense is whitelisted or not. An offense will be considered whitelisted if it is triggered by\n        at least one whitelisted rule\n        :param offense: {Offense} The offense\n        :return: {bool} True if whitelisted, False otherwise\n        \"\"\"\n        self.logger.info(\n            f\"Offense {offense.id} rules: {', '.join([str(rule_id) for rule_id in offense.rule_ids])}\"\n        )\n\n        for rule_id in offense.rule_ids:\n            if self.is_whitelisted_rule_id(rule_id):\n                return True\n\n        return False\n\n    def is_passed_whitelist_filters(self, offense):\n        \"\"\"\n        Check if the offense is whitelisted or not. An offense will be considered whitelisted if it is triggered by\n        at least one whitelisted rule\n        :param offense: {Offense} The offense\n        :return: {bool} True if whitelisted, False otherwise\n        \"\"\"\n        self.logger.info(\n            f\"Offense {offense.id} rules: {', '.join([str(rule_id) for rule_id in offense.rule_ids])}\"\n        )\n\n        for rule_id in offense.rule_ids:\n            if self.is_whitelisted_rule_id(rule_id):\n                return True\n\n        return False\n\n    def is_approaching_timeout(self):\n        \"\"\"\n        Check if a timeout is approaching.\n        :return: {bool} True if timeout is close, False otherwise\n        \"\"\"\n        processing_time_ms = unix_now() - self.connector_starting_time\n        return (\n            processing_time_ms\n            > self.params.python_process_timeout * 1000 * TIMEOUT_THRESHOLD\n        )\n\n    def process_alerts(self, fetched_offenses, is_test_run):\n        \"\"\"\n        This method handles the processing of fetched offenses.\n        For each offense, processes the offense and creates AlertInfo for each whitelisted rule in the offense that\n        had events.\n        This method had to be overwritten because as opposed to normal connectors that create a single AlertInfo per\n        alert, the QRadar connector creates multiple AlertInfo, per rule in the offense.\n        :param fetched_offenses: {[Offense]} The fetched offenses\n        :param is_test_run: {bool} Whether the current run of the connector is a test run or not\n        :return: ([AlertInfo], [AlertInfo]) All created AlertInfo objects, only the processed AlertInfo (non-overflowed)\n        \"\"\"\n        all_alerts = []\n        processed_alerts = []\n        processed_offenses_count = 0\n\n        for index, fetched_offense in enumerate(fetched_offenses):\n            try:\n                if self.is_approaching_timeout():\n                    self.logger.info(\n                        \"Timeout is approaching. Connector will gracefully exit.\"\n                    )\n                    break\n\n                # Perform pre processing on the current alert\n                self.alert_pre_processing(fetched_offense)\n\n                if not self.is_passed_whitelist_filters(fetched_offense):\n                    self.logger.info(\n                        \"Offense {} did not pass whitelist filter. Skipping.\".format(\n                            self.alert_id_repr(fetched_offense) or f\"#{index}\"\n                        )\n                    )\n                    continue\n\n                # Process a single alert - create an AlertInfo\n                alert_infos = self.process_alert(fetched_offense, index)\n\n                if alert_infos:\n                    processed_offenses_count += 1\n\n                for alert_info in alert_infos:\n                    # Determine if the alert is an overflowed alert or not\n                    is_overflow = (\n                        self.is_overflow_alert(alert_info, is_test_run)\n                        if not self.params.disable_overflow\n                        else False\n                    )\n\n                    if not is_overflow:\n                        # The alert is not an overflow - add it to processed alert\n                        processed_alerts.append(alert_info)\n\n                    # Perform post processing on the current alert\n                    self.alert_post_processing(fetched_offense, alert_info, is_overflow)\n\n                    all_alerts.append(alert_info)\n\n                self.logger.info(\n                    \"Finished processing offense {}.\".format(\n                        self.alert_id_repr(fetched_offense) or f\"#{index}\"\n                    )\n                )\n\n                self.logger.debug(\n                    \"Offense {} last update time: {}\".format(\n                        self.alert_id_repr(fetched_offense) or f\"#{index}\",\n                        convert_unixtime_to_datetime(\n                            fetched_offense.last_updated_time\n                        ).isoformat(),\n                    )\n                )\n                self.processed_offenses.append(fetched_offense)\n\n                if processed_offenses_count >= self.params.offenses_limit_per_cycle:\n                    self.logger.info(\n                        \"Reached max amount of offenses per cycle limit. Aborting.\"\n                    )\n                    break\n\n                if is_test_run:\n                    self.logger.info(\"This is a TEST run. Only 1 offense is processed.\")\n                    break\n\n            except Exception as e:\n                self.logger.error(\n                    \"Failed to process offense {}\".format(\n                        self.alert_id_repr(fetched_offense) or f\"#{index}\"\n                    )\n                )\n                if isinstance(e, QRadarCustomFieldValidation):\n                    raise Exception(\n                        \"Connector failed to run because provided Custom Fields caused Qradar AQL query \"\n                        \"validation error. Please make sure that the Custom Fields are provided without \"\n                        \"errors and exist in Qradar events table.\"\n                    )\n                self.logger.exception(e)\n\n                if is_test_run:\n                    raise\n\n        return all_alerts, processed_alerts\n\n    def process_alert(self, offense, index):\n        \"\"\"\n        This method handles the processing of a single offense.\n        This method fetches the events of the offense for each whitelist rule of the offense and creates an AlertInfo\n        out of the offense and the matching rule and its events.\n        This method had to be overwritten because as opposed to normal connectors, QRadar connector has another layer\n        - alerts (offenses), rules and events.\n        :param offense: The fetched offense\n        :param index: {int} The index of the alert within all the fetched alerts in the current connector cycle\n        :return: {AlertInfo} The created AlertInfo for the alert\n        \"\"\"\n        processed_alerts = []\n        total_events_count = 0\n\n        self.logger.info(\n            \"Processing offense {}\".format(self.alert_id_repr(offense) or f\"#{index}\")\n        )\n\n        for rule_id in offense.rule_ids:\n            if self.is_approaching_timeout():\n                # Need to gracefully exit\n                break\n\n            if self.is_rule_id_passed_whitelist_filter(rule_id):\n                rule_name = self.get_rule_name_by_id(rule_id)\n\n                try:\n                    if self.is_events_limit_per_offense_rule_reached(\n                        offense.id, rule_id\n                    ):\n                        self.logger.info(\n                            f\"Events limit for {str(offense.id)} offense {rule_id} {rule_name} rule is \"\n                            f\"reached. Skipping\"\n                        )\n                        continue\n\n                    self.logger.info(\n                        \"Fetching events of offense {}, rule: {} ({})\".format(\n                            self.alert_id_repr(offense) or f\"#{index}\",\n                            rule_name,\n                            rule_id,\n                        )\n                    )\n\n                    # Fetch the events of the offense for the current rule\n                    events = self.get_events_for_rule(\n                        offense,\n                        rule_id,\n                        self.calculate_events_min_limit(offense.id, rule_id),\n                        self.get_already_seen_events_hashes(offense, rule_id),\n                    )\n\n                    for event in events:\n                        self.save_event_to_offense_events(offense, rule_id, event)\n\n                    events_len = len(events)\n                    total_events_count += events_len\n\n                    if events:\n                        self.logger.info(\n                            f\"Found {events_len} events for rule {rule_name} ({rule_id}). Stopping timer.\"\n                        )\n                        self.logger.info(\n                            f\"Creating an AlertInfo for rule {rule_name} ({rule_id})\"\n                        )\n                        # Create an AlertInfo object for the offense and rule\n                        processed_alerts.append(\n                            self.create_alert_info(offense, events, rule_name)\n                        )\n\n                        # Stop the \"failed to fetch events\" timer for the offense\n                        self.stop_offense_timer(offense.id)\n\n                    else:\n                        self.logger.info(\n                            f\"No events were found for rule {rule_name} ({rule_id})\"\n                        )\n                        continue\n\n                    self.logger.info(\n                        f\"Finished processing rule {rule_name} ({rule_id}) of offense {offense.id}\"\n                    )\n\n                except Exception as e:\n                    # Start the \"failed to fetch events\" timer for the offense.\n                    self.start_offense_timer(offense.id)\n                    self.logger.error(\n                        \"Failed fetching events of alert {} for rule {}. Starting timer.\".format(\n                            self.alert_id_repr(offense) or f\"#{index}\", rule_name\n                        )\n                    )\n                    if isinstance(e, QRadarCustomFieldValidation):\n                        raise QRadarCustomFieldValidation()\n                    self.logger.exception(e)\n\n            else:\n                self.logger.info(\n                    f\"Rule {rule_id} did not pass whitelist filter. Skipping\"\n                )\n\n        if not total_events_count:\n            # TODO: maybe we need to remove this one as this will cause many false positives\n            # No events were found for any of the whitelisted rules - starting timer.\n            # If for a long period (current offense padding period), there will be no new events, we will\n            self.logger.info(\n                \"No events were found for offense {} from all rules. Starting timer.\".format(\n                    self.alert_id_repr(offense) or f\"#{index}\"\n                )\n            )\n            self.start_offense_timer(offense.id)\n\n        return processed_alerts\n\n    def get_events_for_rule(\n        self, offense, rule_id, limit=None, existing_events_hashes=[]\n    ):\n        \"\"\"\n        Get events for an offense and a specific rule\n        :param offense: {Offense} The offense to fetch events for\n        :param rule_id: {int} The rule id to fetch events for\n        :param limit: {int} Max amount of events to fetch\n        :param existing_events_hashes: {[]} List of already seen events hashes\n        :return: {[Event]} List of events\n        \"\"\"\n        return self.manager.get_events_by_offense_id(\n            offense_id=offense.id,\n            log_source_ids=[\n                str(log_source_id) for log_source_id in offense.log_source_ids\n            ],\n            rules_ids=[str(rule_id)],\n            custom_fields=self.params.custom_fields,\n            events_period_padding=self.params.events_padding_period,\n            limit=limit,\n            existing_events_hashes=existing_events_hashes,\n            page_size=self.params.events_page_size,\n            total_limit_of_events_per_offense=self.params.events_query_limit,\n            order_by_key=DEFAULT_ORDER_BY_KEY,\n            sort_order=DEFAULT_SORT_ORDER,\n        )\n\n    def create_alert_info(self, offense, events, rule_name=None):\n        \"\"\"\n        Creatw an AlertInfo object from a single alert and its activities\n        :param offense: {Offense} An offense instance\n        :param rule_name: {unicode} The rule name that triggered the events\n        :param events: [Event] A list of the events objects related to the offense\n        :return: {AlertInfo} The created alert info object\n        \"\"\"\n        alert_info = AlertInfo()\n\n        # Set the times of the AlertInfo based on the oldest and newest events in it\n        events = sorted(events, key=lambda item: item.start_time or 1)\n        alert_info.start_time = int(events[0].start_time or 1)\n        alert_info.end_time = int(events[-1].end_time or 1)\n\n        alert_info.ticket_id = (\n            f\"{offense.id}_{rule_name}_{alert_info.start_time}_{alert_info.end_time}\"\n        )\n        alert_info.display_id = f\"{alert_info.ticket_id}_{uuid.uuid4()}\"\n        alert_info.name = (\n            rule_name\n            if self.params.alert_name_field_name == \"custom_rule\"\n            else offense.description\n        )\n        alert_info.rule_generator = (\n            rule_name\n            if self.params.rule_generator_field_name == \"custom_rule\"\n            else offense.description\n        )\n        alert_info.priority = offense.priority\n        alert_info.description = f\"Offence ID: {offense.id}, Rule Name: {rule_name}\"\n        alert_info.device_product = (\n            events[0].device_product if events else \"Error Getting Device Product\"\n        )\n        alert_info.device_vendor = self.common.get_category_human_readable_value(\n            events[0].category if events else None\n        )\n        alert_info.environment = self.environment_common.get_environment(\n            offense.as_extension()\n        )\n        alert_info.source_grouping_identifier = offense.id\n        alert_info.extensions.update(offense.as_extension())\n        alert_info.extensions.update({\"rule_name\": rule_name, \"offense_id\": offense.id})\n\n        # Flat events data.\n        try:\n            alert_info.events = [event.as_event() for event in events]\n        except Exception as e:\n            self.logger.error(f\"Unable to flatten events: {e}\")\n            self.logger.exception(e)\n            alert_info.events = []\n\n        return alert_info\n\n    def create_failed_to_fetch_events_alert_info(self, offense_id):\n        \"\"\"\n        Create an \"empty\" AlertInfo object for an offense that we failed to fetch events for (for a long time)\n        :param offense_id: {int} The ID of the offense to create the empty alert for\n        :return: {AlertInfo} The created alert info object\n        \"\"\"\n        alert_info = AlertInfo()\n        alert_info.start_time = 1\n        alert_info.end_time = 1\n\n        alert_info.ticket_id = f\"{offense_id}_{FAILED_TO_FETCH_EVENTS}_{alert_info.start_time}_{alert_info.end_time}\"\n        alert_info.display_id = f\"{alert_info.ticket_id}_{uuid.uuid4()}\"\n        alert_info.name = FAILED_TO_FETCH_EVENTS\n        alert_info.rule_generator = FAILED_TO_FETCH_EVENTS\n        alert_info.priority = -1\n        alert_info.description = (\n            f\"Offence ID: {offense_id}, Rule Name: {FAILED_TO_FETCH_EVENTS}\"\n        )\n        alert_info.device_product = \"Error Getting Device Product\"\n        alert_info.device_vendor = self.common.DEFAULT_CATEGORY\n        alert_info.environment = self.siemplify.context.connector_info.environment\n        alert_info.source_grouping_identifier = offense_id\n        alert_info.events = []\n        alert_info.extensions.update(\n            {\"rule_name\": self.FAILED_TO_FETCH_EVENTS, \"offense_id\": offense_id}\n        )\n        return alert_info\n\n    def is_events_limit_per_offense_rule_reached(self, offense_id, rule_id):\n        \"\"\"\n        Check whether enough amount of events were already fetched for the offense rule\n        :param offense_id: {int} The ID of the offense\n        :param rule_id: {int} The ID of the offense rule\n        :return: True if already reached limit, False otherwise\n        \"\"\"\n        if not self.params.offense_rule_events_limit:\n            return False\n\n        total_events_per_rule = (\n            self.offense_events.get(\"offenses\", {})\n            .get(str(offense_id), {})\n            .get(\"total_events_collected_per_rule\", {})\n            .get(str(rule_id), 0)\n        )\n\n        if (\n            total_events_per_rule\n            and total_events_per_rule >= self.params.offense_rule_events_limit\n        ):\n            return True\n\n        return False\n\n    def calculate_events_min_limit(self, offense_id, rule_id):\n        \"\"\"\n        Calculate events min limit based on events_limit_per_alert and offense_rule_events_limit parameters\n        :param offense_id: {int} The ID of the offense\n        :param rule_id: {int} The ID of the offense rule\n        :return: {int} min limit for events\n        \"\"\"\n        if not self.params.offense_rule_events_limit:\n            return self.params.events_limit_per_alert\n\n        total_events_per_rule = (\n            self.offense_events.get(\"offenses\", {})\n            .get(str(offense_id), {})\n            .get(\"total_events_collected_per_rule\", {})\n            .get(str(rule_id), 0)\n        )\n\n        return min(\n            self.params.events_limit_per_alert,\n            self.params.offense_rule_events_limit - total_events_per_rule,\n        )\n\n\n@output_handler\ndef main():\n    is_test_run = not (len(sys.argv) < 2 or sys.argv[1] == \"True\")\n    connector = QradarCorrelationEventsConnectorV2()\n    connector.run(is_test_run, process_single_alert_on_test=False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "documentationLink": "https://cloud.google.com/chronicle/docs/soar/marketplace-integrations/qradar#qradar-correlation-events-connector-v2",
    "deviceProductField": "deviceProduct",
    "eventNameField": "EventName",
    "connectorStatus": {
        "connectorIdentifier": "Qradar Correlation Events Connector V2_c3b0b6dd-6a39-4bfb-9799-8af6944358d1",
        "connectivityStatus": 1,
        "amountAlertsInLastDay": 0,
        "avgAlertsPerDay": 4
    },
    "isNew": false,
    "agentIdentifier": "821d00e4-c027-4ae6-a6d1-fd3326fccd03"
}